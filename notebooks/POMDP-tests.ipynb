{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/JuliaPOMDP/POMDPs.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Cloning\u001b[22m\u001b[39m registry from \"https://github.com/JuliaPOMDP/Registry\"\n",
      "\u001b[2K\u001b[?25h\u001b[32m\u001b[1m     Added\u001b[22m\u001b[39m registry `JuliaPOMDP` to `~/.julia/registries`0 %========>                                ]  18.3 %22.2 %>                       ]  40.5 % %>              ]  64.4 % [==============================>          ]  73.9 %   ]  92.0 %\u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=======================================> ]  96.8 %\n"
     ]
    }
   ],
   "source": [
    "POMDPs.add_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %\u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=======================================> ]  96.7 %\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/JuliaPOMDP`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaPOMDP/Registry`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m QuickPOMDPs ───── v0.1.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m POMDPTesting ──── v0.1.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m BeliefUpdaters ── v0.1.2\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m POMDPModelTools ─ v0.1.6\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Project.toml`\n",
      " \u001b[90m [8af83fb2]\u001b[39m\u001b[92m + QuickPOMDPs v0.1.0\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Manifest.toml`\n",
      " \u001b[90m [8bb6e9a1]\u001b[39m\u001b[92m + BeliefUpdaters v0.1.2\u001b[39m\n",
      " \u001b[90m [08074719]\u001b[39m\u001b[92m + POMDPModelTools v0.1.6\u001b[39m\n",
      " \u001b[90m [92e6a534]\u001b[39m\u001b[92m + POMDPTesting v0.1.0\u001b[39m\n",
      " \u001b[90m [8af83fb2]\u001b[39m\u001b[92m + QuickPOMDPs v0.1.0\u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m TableTraits ───────────────── v1.0.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m InvertedIndices ───────────── v1.0.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m DataValueInterfaces ───────── v1.0.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m Tables ────────────────────── v0.2.10\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m ProgressMeter ─────────────── v0.9.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m POMDPSimulators ───────────── v0.2.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m CategoricalArrays ─────────── v0.5.5\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m DataFrames ────────────────── v0.19.1\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m IteratorInterfaceExtensions ─ v1.0.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m PooledArrays ──────────────── v0.5.2\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m DataAPI ───────────────────── v1.0.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m POMDPPolicies ─────────────── v0.1.5\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Project.toml`\n",
      " \u001b[90m [e0d0a172]\u001b[39m\u001b[92m + POMDPSimulators v0.2.0\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Manifest.toml`\n",
      " \u001b[90m [324d7699]\u001b[39m\u001b[92m + CategoricalArrays v0.5.5\u001b[39m\n",
      " \u001b[90m [9a962f9c]\u001b[39m\u001b[92m + DataAPI v1.0.0\u001b[39m\n",
      " \u001b[90m [a93c6f00]\u001b[39m\u001b[92m + DataFrames v0.19.1\u001b[39m\n",
      " \u001b[90m [e2d170a0]\u001b[39m\u001b[92m + DataValueInterfaces v1.0.0\u001b[39m\n",
      " \u001b[90m [41ab1584]\u001b[39m\u001b[92m + InvertedIndices v1.0.0\u001b[39m\n",
      " \u001b[90m [82899510]\u001b[39m\u001b[92m + IteratorInterfaceExtensions v1.0.0\u001b[39m\n",
      " \u001b[90m [182e52fb]\u001b[39m\u001b[92m + POMDPPolicies v0.1.5\u001b[39m\n",
      " \u001b[90m [e0d0a172]\u001b[39m\u001b[92m + POMDPSimulators v0.2.0\u001b[39m\n",
      " \u001b[90m [2dfb63ee]\u001b[39m\u001b[92m + PooledArrays v0.5.2\u001b[39m\n",
      " \u001b[90m [92933f4c]\u001b[39m\u001b[92m + ProgressMeter v0.9.0\u001b[39m\n",
      " \u001b[90m [3783bdb8]\u001b[39m\u001b[92m + TableTraits v1.0.0\u001b[39m\n",
      " \u001b[90m [bd369af6]\u001b[39m\u001b[92m + Tables v0.2.10\u001b[39m\n",
      " \u001b[90m [9fa8497b]\u001b[39m\u001b[92m + Future \u001b[39m\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m DiscreteValueIteration ─ v0.3.0\n",
      "\u001b[32m\u001b[1m   Cloning\u001b[22m\u001b[39m [3aa3ecc9-5a5d-57c8-8188-3e47bd8068d2] QMDP from https://github.com/JuliaPOMDP/QMDP.jl\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %]  27.8 %========================>                ]  58.0 % [=========================>               ]  61.5 %   ]  91.5 % ]  97.3 %\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Project.toml`\n",
      " \u001b[90m [3aa3ecc9]\u001b[39m\u001b[92m + QMDP v0.1.1\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Manifest.toml`\n",
      " \u001b[90m [4b033969]\u001b[39m\u001b[92m + DiscreteValueIteration v0.3.0\u001b[39m\n",
      " \u001b[90m [3aa3ecc9]\u001b[39m\u001b[92m + QMDP v0.1.1\u001b[39m\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m QMDP → `~/.julia/packages/QMDP/yrO4U/deps/build.log`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Installed QMDP ─────────────────── v0.1.1\n",
      "└ @ Pkg.Operations /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.1/Pkg/src/Operations.jl:637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Nothing,1}:\n",
       " nothing\n",
       " nothing\n",
       " nothing"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pkg.add.([\"QuickPOMDPs\", \"POMDPSimulators\", \"QMDP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling QuickPOMDPs [8af83fb2-a731-493c-9049-9e19dbce6165]\n",
      "└ @ Base loading.jl:1186\n",
      "┌ Info: Precompiling POMDPSimulators [e0d0a172-29c6-5d4e-96d0-f262df5d01fd]\n",
      "└ @ Base loading.jl:1186\n",
      "┌ Info: Precompiling QMDP [3aa3ecc9-5a5d-57c8-8188-3e47bd8068d2]\n",
      "└ @ Base loading.jl:1186\n"
     ]
    }
   ],
   "source": [
    "using POMDPs, QuickPOMDPs, POMDPSimulators, QMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscreteExplicitPOMDP{Symbol,Symbol,Symbol,typeof(Z),typeof(R)}(Symbol[:left, :right], Symbol[:left, :right, :listen], Symbol[:left, :right], Dict((:left, :right)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:left, :listen)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left], [1.0]),(:right, :left)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:right, :right)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:right, :listen)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:right], [1.0]),(:left, :left)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5])), Dict((:listen, :right)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.15, 0.85]),(:left, :right)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:right, :left)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:right, :right)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:listen, :left)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.85, 0.15]),(:left, :left)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5])), Z, R, Dict(:left=>1,:right=>2), Dict(:left=>1,:right=>2,:listen=>3), Dict(:left=>1,:right=>2), 0.95)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = [:left, :right]           # S, A, and O may contain any objects\n",
    "A = [:left, :right, :listen]  # including user-defined types\n",
    "O = [:left, :right]\n",
    "γ = 0.95\n",
    "\n",
    "function T(s, a, sp)\n",
    "    if a == :listen\n",
    "        return s == sp\n",
    "    else # a door is opened\n",
    "        return 0.5 #reset\n",
    "    end\n",
    "end\n",
    "\n",
    "function Z(a, sp, o)\n",
    "    if a == :listen\n",
    "        if o == sp\n",
    "            return 0.85\n",
    "        else\n",
    "            return 0.15\n",
    "        end\n",
    "    else\n",
    "        return 0.5\n",
    "    end\n",
    "end\n",
    "\n",
    "function R(s, a)\n",
    "    if a == :listen  \n",
    "        return -1.0\n",
    "    elseif s == a # the tiger was found\n",
    "        return -100.0\n",
    "    else # the tiger was escaped\n",
    "        return 10.0\n",
    "    end\n",
    "end\n",
    "\n",
    "m = DiscreteExplicitPOMDP(S,A,O,T,Z,R,γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POMDPPolicies.AlphaVectorPolicy{DiscreteExplicitPOMDP{Symbol,Symbol,Symbol,typeof(Z),typeof(R)},Symbol}(DiscreteExplicitPOMDP{Symbol,Symbol,Symbol,typeof(Z),typeof(R)}(Symbol[:left, :right], Symbol[:left, :right, :listen], Symbol[:left, :right], Dict((:left, :right)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:left, :listen)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left], [1.0]),(:right, :left)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:right, :right)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:right, :listen)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:right], [1.0]),(:left, :left)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5])), Dict((:listen, :right)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.15, 0.85]),(:left, :right)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:right, :left)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:right, :right)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5]),(:listen, :left)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.85, 0.15]),(:left, :left)=>SparseCat{Array{Symbol,1},Array{Float64,1}}(Symbol[:left, :right], [0.5, 0.5])), Z, R, Dict(:left=>1,:right=>2), Dict(:left=>1,:right=>2,:listen=>3), Dict(:left=>1,:right=>2), 0.95), Array{Float64,1}[[89.7766, 199.784], [199.777, 89.7841], [188.773, 188.78]], Symbol[:left, :right, :listen])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver = QMDPSolver()\n",
    "policy = solve(solver, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: left, b: [0.5, 0.5], a: listen, o: left\n",
      "s: left, b: [0.85, 0.15], a: listen, o: left\n",
      "s: left, b: [0.969799, 0.0302013], a: right, o: left\n",
      "s: left, b: [0.5, 0.5], a: listen, o: left\n",
      "s: left, b: [0.85, 0.15], a: listen, o: left\n",
      "s: left, b: [0.969799, 0.0302013], a: right, o: left\n",
      "s: right, b: [0.5, 0.5], a: listen, o: right\n",
      "s: right, b: [0.15, 0.85], a: listen, o: right\n",
      "s: right, b: [0.0302013, 0.969799], a: left, o: right\n",
      "s: left, b: [0.5, 0.5], a: listen, o: right\n",
      "Undiscounted reward was 23.0.\n"
     ]
    }
   ],
   "source": [
    "rsum = 0.0\n",
    "for (s,b,a,o,r) in stepthrough(m, policy, \"s,b,a,o,r\", max_steps=10)\n",
    "    println(\"s: $s, b: $([pdf(b,s) for s in S]), a: $a, o: $o\")\n",
    "    global rsum += r\n",
    "end\n",
    "println(\"Undiscounted reward was $rsum.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Status\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Project.toml`\n",
      " \u001b[90m [587475ba]\u001b[39m\u001b[37m Flux v0.8.3\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.status(\"Flux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m Arpack ──────────→ `~/.julia/packages/Arpack/cu5By/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m SpecialFunctions → `~/.julia/packages/SpecialFunctions/fvheQ/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m Rmath ───────────→ `~/.julia/packages/Rmath/Py9gH/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m ZMQ ─────────────→ `~/.julia/packages/ZMQ/ABGOx/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m ZipFile ─────────→ `~/.julia/packages/ZipFile/oD4uG/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m CodecZlib ───────→ `~/.julia/packages/CodecZlib/9jDi1/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m Conda ───────────→ `~/.julia/packages/Conda/kLXeC/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m FFTW ────────────→ `~/.julia/packages/FFTW/2okGQ/deps/build.log`\n"
     ]
    }
   ],
   "source": [
    "Pkg.clone(\"DeepQLearning\")\n",
    "Pkg.build(\"DeepQLearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling DeepQLearning [de0a67f4-c691-11e8-0034-5fc6e16e22d3]\n",
      "└ @ Base loading.jl:1186\n"
     ]
    }
   ],
   "source": [
    "using DeepQLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m Measures ───── v0.3.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m IterTools ──── v1.2.0\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m Compose ────── v0.7.3\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m POMDPModels ── v0.3.5\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m ColorSchemes ─ v3.3.0\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Project.toml`\n",
      " \u001b[90m [355abbd5]\u001b[39m\u001b[92m + POMDPModels v0.3.5\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Manifest.toml`\n",
      " \u001b[90m [35d6a980]\u001b[39m\u001b[92m + ColorSchemes v3.3.0\u001b[39m\n",
      " \u001b[90m [a81c6b42]\u001b[39m\u001b[92m + Compose v0.7.3\u001b[39m\n",
      " \u001b[90m [c8e1da08]\u001b[39m\u001b[92m + IterTools v1.2.0\u001b[39m\n",
      " \u001b[90m [442fdcdd]\u001b[39m\u001b[92m + Measures v0.3.0\u001b[39m\n",
      " \u001b[90m [355abbd5]\u001b[39m\u001b[92m + POMDPModels v0.3.5\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.add([\"POMDPSimulators\", \"POMDPModels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling POMDPModels [355abbd5-f08e-5560-ac9e-8b5f2592a0ca]\n",
      "└ @ Base loading.jl:1186\n",
      "WARNING: Method definition GridWorld(Any...) in module POMDPModels at deprecated.jl:31 overwritten at deprecated.jl:55.\n"
     ]
    }
   ],
   "source": [
    "using DeepQLearning\n",
    "using POMDPs\n",
    "using Flux\n",
    "using POMDPModels\n",
    "using POMDPSimulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MDP model from POMDPModels or define your own!\n",
    "mdp = SimpleGridWorld();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(2, 32), Dense(32, 4))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Q network (see Flux.jl documentation)\n",
    "# the gridworld state is represented by a 2 dimensional vector.\n",
    "model = Chain(Dense(2, 32), Dense(32, n_actions(mdp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepQLearningSolver\n",
       "  qnetwork: Chain{Tuple{Dense{typeof(identity),TrackedArray{…,Array{Float32,2}},TrackedArray{…,Array{Float32,1}}},Dense{typeof(identity),TrackedArray{…,Array{Float32,2}},TrackedArray{…,Array{Float32,1}}}}}\n",
       "  learning_rate: Float64 0.005\n",
       "  max_steps: Int64 10000\n",
       "  batch_size: Int64 32\n",
       "  train_freq: Int64 4\n",
       "  eval_freq: Int64 500\n",
       "  target_update_freq: Int64 500\n",
       "  num_ep_eval: Int64 100\n",
       "  double_q: Bool true\n",
       "  dueling: Bool true\n",
       "  recurrence: Bool false\n",
       "  eps_fraction: Float64 0.5\n",
       "  eps_end: Float64 0.01\n",
       "  evaluation_policy: DeepQLearning.basic_evaluation (function of type typeof(DeepQLearning.basic_evaluation))\n",
       "  exploration_policy: getfield(DeepQLearning, Symbol(\"#action_epsilon_greedy#11\")){Int64,Float64,Float64}(10000, 0.5, 0.01) (function of type getfield(DeepQLearning, Symbol(\"#action_epsilon_greedy#11\")){Int64,Float64,Float64})\n",
       "  trace_length: Int64 40\n",
       "  prioritized_replay: Bool true\n",
       "  prioritized_replay_alpha: Float64 0.6\n",
       "  prioritized_replay_epsilon: Float64 1.0e-6\n",
       "  prioritized_replay_beta: Float64 0.4\n",
       "  buffer_size: Int64 1000\n",
       "  max_episode_length: Int64 100\n",
       "  train_start: Int64 200\n",
       "  rng: Random.MersenneTwister\n",
       "  logdir: String \"log/\"\n",
       "  save_freq: Int64 3000\n",
       "  log_freq: Int64 500\n",
       "  verbose: Bool true\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver = DeepQLearningSolver(qnetwork = model, max_steps=10000, \n",
    "                             learning_rate=0.005,log_freq=500,\n",
    "                             recurrence=false,double_q=true, dueling=true, prioritized_replay=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  500 / 10000 eps 0.901 |  avgR -0.083 | Loss 2.005e+00 | Grad 2.324e-01 | EvalR -Inf \n",
      "Evaluation ... Avg Reward 1.14 | Avg Step 78.11 \n",
      " 1000 / 10000 eps 0.802 |  avgR 0.412 | Loss 1.306e+00 | Grad 1.005e-01 | EvalR 1.140 \n",
      "Evaluation ... Avg Reward -4.47 | Avg Step 51.14 \n",
      " 1500 / 10000 eps 0.703 |  avgR 0.000 | Loss 1.206e+00 | Grad 2.229e-01 | EvalR -4.470 \n",
      "Evaluation ... Avg Reward 2.20 | Avg Step 71.12 \n",
      " 2000 / 10000 eps 0.604 |  avgR 0.473 | Loss 6.163e-01 | Grad 1.213e-01 | EvalR 2.200 \n",
      "Evaluation ... Avg Reward -1.44 | Avg Step 48.51 \n",
      " 2500 / 10000 eps 0.505 |  avgR 0.183 | Loss 5.707e-01 | Grad 7.312e-02 | EvalR -1.440 \n",
      "Evaluation ... Avg Reward 3.54 | Avg Step 48.56 \n",
      " 3000 / 10000 eps 0.406 |  avgR -0.100 | Loss 6.952e-01 | Grad 2.380e-01 | EvalR 3.540 \n",
      "Evaluation ... Avg Reward 1.20 | Avg Step 60.61 \n",
      "Saving new model with eval reward 1.200 \n",
      " 3500 / 10000 eps 0.307 |  avgR -0.522 | Loss 7.535e-01 | Grad 7.019e-02 | EvalR 1.200 \n",
      "Evaluation ... Avg Reward 2.41 | Avg Step 56.72 \n",
      " 4000 / 10000 eps 0.208 |  avgR -0.069 | Loss 5.071e-01 | Grad 9.680e-02 | EvalR 2.410 \n",
      "Evaluation ... Avg Reward 1.24 | Avg Step 61.88 \n",
      " 4500 / 10000 eps 0.109 |  avgR 0.618 | Loss 4.396e-01 | Grad 2.581e-01 | EvalR 1.240 \n",
      "Evaluation ... Avg Reward 2.95 | Avg Step 46.52 \n",
      " 5000 / 10000 eps 0.010 |  avgR 0.569 | Loss 2.169e-01 | Grad 1.711e-01 | EvalR 2.950 \n",
      "Evaluation ... Avg Reward -4.98 | Avg Step 41.32 \n",
      " 5500 / 10000 eps 0.010 |  avgR 0.147 | Loss 3.268e-01 | Grad 2.671e-01 | EvalR -4.980 \n",
      "Evaluation ... Avg Reward 4.39 | Avg Step 44.30 \n",
      " 6000 / 10000 eps 0.010 |  avgR 0.529 | Loss 5.648e-01 | Grad 1.044e-01 | EvalR 4.390 \n",
      "Evaluation ... Avg Reward -0.86 | Avg Step 65.16 \n",
      " 6500 / 10000 eps 0.010 |  avgR 0.176 | Loss 6.136e-01 | Grad 4.108e-02 | EvalR -0.860 \n",
      "Evaluation ... Avg Reward -0.54 | Avg Step 47.30 \n",
      " 7000 / 10000 eps 0.010 |  avgR 0.275 | Loss 7.220e-01 | Grad 2.537e-02 | EvalR -0.540 \n",
      "Evaluation ... Avg Reward -0.42 | Avg Step 64.54 \n",
      " 7500 / 10000 eps 0.010 |  avgR 0.559 | Loss 2.784e-01 | Grad 2.230e-02 | EvalR -0.420 \n",
      "Evaluation ... Avg Reward -0.11 | Avg Step 76.95 \n",
      " 8000 / 10000 eps 0.010 |  avgR 0.843 | Loss 2.982e-01 | Grad 1.965e-02 | EvalR -0.110 \n",
      "Evaluation ... Avg Reward 1.61 | Avg Step 68.21 \n",
      " 8500 / 10000 eps 0.010 |  avgR 0.784 | Loss 2.893e-01 | Grad 1.649e-02 | EvalR 1.610 \n",
      "Evaluation ... Avg Reward 1.90 | Avg Step 61.99 \n",
      " 9000 / 10000 eps 0.010 |  avgR 0.824 | Loss 3.813e-01 | Grad 1.708e-02 | EvalR 1.900 \n",
      "Evaluation ... Avg Reward 3.27 | Avg Step 67.02 \n",
      "Saving new model with eval reward 3.270 \n",
      " 9500 / 10000 eps 0.010 |  avgR 0.667 | Loss 3.600e-01 | Grad 1.739e-01 | EvalR 3.270 \n",
      "Evaluation ... Avg Reward 3.57 | Avg Step 65.69 \n",
      "10000 / 10000 eps 0.010 |  avgR 1.078 | Loss 2.005e-01 | Grad 9.529e-03 | EvalR 3.570 \n",
      "Restore model with eval reward 3.270 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NNPolicy{SimpleGridWorld,DeepQLearning.DuelingNetwork,Symbol}(SimpleGridWorld\n",
       "  size: Tuple{Int64,Int64}\n",
       "  rewards: Dict{StaticArrays.SArray{Tuple{2},Int64,1,2},Float64}\n",
       "  terminate_from: Set{StaticArrays.SArray{Tuple{2},Int64,1,2}}\n",
       "  tprob: Float64 0.7\n",
       "  discount: Float64 0.95\n",
       ", DeepQLearning.DuelingNetwork(Chain(), Chain(Dense(2, 32), Dense(32, 1)), Chain(Dense(2, 32), Dense(32, 4))), Symbol[:up, :down, :left, :right], 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = solve(solver, mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total discounted reward for 1 simulation: 2.7075\n"
     ]
    }
   ],
   "source": [
    "sim = RolloutSimulator(max_steps=30)\n",
    "r_tot = simulate(sim, mdp, policy)\n",
    "println(\"Total discounted reward for 1 simulation: $r_tot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
